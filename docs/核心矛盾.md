# DeepSeek VLM 自评估机制的核心矛盾

## 问题的本质

在 NL2SQL 实现中存在一个核心矛盾：**如果模型真的知道哪里错了，为什么不一开始就生成正确的SQL？**

---

## 代码中的自评估机制

### 1. 通过提示模板"强迫"模型评估

```python
# 从 templates.py
"""你是一个资深的数据分析师...你需要完成以下步骤：

3. 【自我评估】生成SQL后，进行严格的自检：
   - 检查语法是否正确
   - 验证逻辑是否符合问题要求
   - 确认所有必要的条件都已包含
   - 评估可能存在的问题

你的回答必须严格按照以下格式：

<self_eval>
[你的自我评估结果...]
</self_eval>
"""
```

模型被**要求**输出自评估，所以它就"假装"在检查问题。

---

### 2. 模型输出的是"形式上的检查"，不是真正的认知

```
用户问题：查询今年净线索的数量

模型生成的SQL：SELECT COUNT(*) FROM leads WHERE is_net_leads=1

模型的self_eval：
"语法正确性：✓
逻辑完整性：⚠️ 缺少时间条件
潜在问题：没有year_id过滤
置信度：0.7"
```

**关键问题**：
- 模型在生成SQL时"忘了"加时间过滤
- 但在自评估部分又"想起来"应该有时间过滤
- 这是同一个模型在两种不同"模式"下的行为

---

## 为什么会这样？

### 温度采样的随机性

```python
# generators.py
outputs = self.model.generate(
    inputs,
    temperature=0.7,  # <--- 关键点！
    top_p=0.95,
    ...
)
```

- `temperature=0.7` 意味着生成是**随机的**
- 同样的输入，每次输出可能不同
- 生成SQL部分和自评估部分是**两次不同的采样**

### 提示工程的"幻觉效应"

模型被问"有潜在问题吗？"时，会被引导去"找问题"：

```
问题：评估潜在问题
模型思维：模板要求我评估，我应该列出一些可能的...
输出：缺少时间过滤，用了SELECT*(性能问题)...
```

即使这些"问题"可能是模型编造的，它也会编出来，因为这是它被要求的。

---

## 这和原论文的区别

| DeepSeekMath-V2 | NL2SQL代码实现 |
|----------------|---------------|
| VLM 独立验证 | 生成器自己验证自己 |
| VLM 是另一个模型，客观性更强 | 同一个模型的两种输出，主观性强 |
| VLM 被训练来判断步骤正确性 | 生成器只是被"要求"输出评估 |
| VLM 的判断有统计意义 | 自评估可能是"瞎写"的 |

**原论文的机制是可信的**，因为：
- VLM 是独立训练的
- VLM 看到的输入是"问题 + 步骤"，不是自己生成的
- VLM 有训练数据教它如何判断对错

**当前代码的机制有缺陷**，因为：
- 同一个模型自我循环论证
- 没有训练数据教模型如何正确自评估
- 依赖提示工程的"引导"，可能不稳定

---

## 代码中如何应对这个问题？

### 1. 把自评估当作"参考"而不是"标准"

```python
# reward_calculator.py
def _calculate_generation_rewards(self, training_history):
    for round_data in training_history:
        generation_data = round_data.get('generation', {})

        # 自评估准确性奖励
        self_eval_accuracy = self._assess_self_eval_accuracy(
            generation_data, round_data
        )

def _assess_self_eval_accuracy(self, generation_data, round_data):
    # 对比自评估分数和验证器的客观分数
    self_eval_score = generation_data.get('self_eval_score', 0.5)

    # 用 SQLVerifier 的分数作为"标准答案"
    verifications = round_data.get('verifications', [])
    objective_score = average([v.get('score') for v in verifications])

    # 如果自评估和客观评分差距小，奖励模型
    return 1.0 - abs(self_eval_score - objective_score)
```

**关键**：代码不信任自评估的正确性，而是用它来判断模型的"自我认知能力"——即模型是否"知道自己懂不懂"。

### 2. 主要依靠 SQLVerifier（规则验证）

```python
# verifiers.py
def verify_sql(self, query, sql, schema, knowledge):
    # 规则验证是主要的
    rule_based_result = self._rule_based_verification(...)

    # 包括：
    # - 语法检查（括号匹配、引号匹配）
    # - 规则检查（JOIN必须有ON、时间关键词要有时间过滤）
    # - 这部分是**确定性的**，不依赖模型主观判断
```

---

## 核心矛盾总结

| 现象 | 解释 |
|------|------|
| 模型知道问题为什么不直接生成正确答案？ | 因为"知道"是假的，是被提示工程"逼"出来的 |
| 自评估可信吗？ | 不可信，代码也不把它当真，只是作为"自我认知"的评估指标 |
| 那为什么还要有自评估？ | 因为DeepSeek论文就是这个结构，这是一种"形式上的继承" |
| 真正可靠的验证是什么？ | SQLVerifier 的规则验证 |

---

```
原论文的"自验证"：独立VLM验证 → 可信
代码的"自评估"：同模型提示输出 → 不太可信
```

---

## 结论

这个实现确实是**简化版**，如果要真正还原论文机制，需要训练一个独立的 SQL 验证模型，而不是依赖提示工程让模型"假装"自我反思。
