"""
GRPOè®­ç»ƒè„šæœ¬
ä½¿ç”¨Group Relative Policy Optimizationè¿›è¡ŒNL2SQLè¿‡ç¨‹å¥–åŠ±å¾®è°ƒ
"""

import os
import sys
import yaml
import logging
import torch
from pathlib import Path
from typing import Optional, Dict
from dataclasses import dataclass, field
from datetime import datetime
from transformers import AutoModelForCausalLM, AutoTokenizer

# Set Hugging Face environment variables to use direct endpoints
os.environ['HF_ENDPOINT'] = 'https://huggingface.co'
os.environ['HF_HUB_URL'] = 'https://huggingface.co'
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
os.environ['HF_HUB_DISABLE_SSL_VERIFICATION'] = '1'

# Force use of local cache for Qwen3-1.7B
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['TRANSFORMERS_CACHE_DIR'] = '/.cache/huggingface/hub/'

# Fix TRL import issues by setting environment variables
os.environ['TRL_USE_RICH'] = 'false'
os.environ['VLLM_WORKER_MULTIPROC_METHOD'] = 'spawn'

# Import TRL with error handling
try:
    from trl import GRPOTrainer, GRPOConfig
    print("âœ… Successfully imported TRL")
except ImportError as e:
    print(f"âŒ Error importing TRL: {e}")
    print("âš ï¸ TRL not available, using mock implementation for testing")
    # Create mock classes for testing when TRL is not available
    class GRPOConfig:
        def __init__(self, **kwargs):
            for k, v in kwargs.items():
                setattr(self, k, v)

    class GRPOTrainer:
        def __init__(self, **kwargs):
            print("âš ï¸ Using mock GRPOTrainer - training will not actually run")

        def train(self):
            print("âš ï¸ Mock training completed")

from accelerate import Accelerator
from peft import LoraConfig, get_peft_model

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from data.data_loader import NL2SQLDataLoader
from reward.reward_model import ProcessRewardModel
from training.train_utils import WandBLogger, CheckpointManager, PerformanceMonitor, Logger, GPUMonitor
from generator.prompts import PromptTemplates
from utils.tensorboard_logger import TensorBoardLogger

logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®æ•°æ®ç±»"""
    # æ¨¡å‹é…ç½®
    model_name: str = "Qwen/Qwen3-1.7B"
    torch_dtype: str = "bfloat16"
    load_in_4bit: bool = False
    load_in_8bit: bool = False

    # æ•°æ®é…ç½®
    train_file: str = ""
    test_file: str = ""
    val_split: float = 0.1

    # è®­ç»ƒé…ç½®
    output_dir: str = "./outputs/checkpoints"
    num_train_epochs: int = 3
    max_steps: int = -1
    per_device_train_batch_size: int = 8
    per_device_eval_batch_size: int = 16
    gradient_accumulation_steps: int = 2
    total_batch_size: int = 32
    learning_rate: float = 7.3e-6
    lr_scheduler_type: str = "cosine"
    warmup_steps: int = 100
    warmup_ratio: float = 0.1
    optim: str = "adamw_8bit"
    max_grad_norm: float = 1.0
    weight_decay: float = 0.01
    bf16: bool = True
    tf32: bool = True

    # GRPOé…ç½®
    num_generations: int = 4
    temperature: float = 0.7
    top_p: float = 0.95
    top_k: int = 50
    max_new_tokens: int = 1024

    # å¥–åŠ±æƒé‡
    type_weight: float = 0.20
    thinking_weight: float = 0.25
    self_assessment_weight: float = 0.25
    sql_structure_weight: float = 0.30

    # è¯„ä¼°é…ç½®
    eval_steps: int = 100
    save_steps: int = 100
    logging_steps: int = 10

    # W&Bé…ç½®
    wandb_enabled: bool = True
    wandb_project: str = "qwen3-nl2sql-grpo"
    wandb_entity: Optional[str] = None

    # ç¡¬ä»¶é…ç½®
    num_gpus: int = 8
    seed: int = 42

    # æ£€æŸ¥ç‚¹é…ç½®
    resume_from_checkpoint: Optional[str] = None
    save_total_limit: int = 3


class NL2SQLTrainer:
    """NL2SQL GRPOè®­ç»ƒå™¨"""

    def __init__(self, config: TrainingConfig):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config
        self._setup_logging()
        self._setup_environment()
        self._initialize_components()

    def _setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        logger_util = Logger(
            log_dir="./outputs/logs",
            log_file="training.log",
            level="INFO"
        )
        logger.info("æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–")

    def _setup_environment(self):
        """è®¾ç½®ç¯å¢ƒ"""
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(self.config.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(self.config.seed)

        # è®°å½•GPUä¿¡æ¯
        GPUMonitor.log_gpu_status()

    def _initialize_components(self):
        """åˆå§‹åŒ–å„ä¸ªç»„ä»¶"""
        logger.info("åˆå§‹åŒ–ç»„ä»¶...")

        # ä»é…ç½®ä¸­è¯»å–TensorBoardè®¾ç½®
        tb_port = getattr(self.config, 'tensorboard_port', 6006)
        auto_start_tb = getattr(self.config, 'auto_start_tensorboard', True)

        # åˆå§‹åŒ–TensorBoardæ—¥å¿—è®°å½•å™¨
        self.tb_logger = TensorBoardLogger(
            log_dir=os.path.join(self.config.output_dir, 'logs'),
            experiment_name=f"nl2sql-grpo-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            port=tb_port,
            auto_start=auto_start_tb
        )

        # åˆå§‹åŒ–W&Bï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.wandb_enabled:
            self.wandb_logger = WandBLogger(
                project=self.config.wandb_project,
                entity=self.config.wandb_entity,
                name=f"nl2sql-grpo-{torch.cuda.get_device_name(0).replace(' ', '-')}",
                config=self.config.__dict__,
                enabled=self.config.wandb_enabled,
                tags=["nl2sql", "grpo", "process_reward", "2025"]
            )
        else:
            logger.info("W&Bå·²ç¦ç”¨ï¼Œä½¿ç”¨TensorBoardè®°å½•")
            self.wandb_logger = None

        # åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.checkpoint_manager = CheckpointManager(
            output_dir=self.config.output_dir,
            save_total_limit=self.config.save_total_limit,
            best_model_metric="eval_type_accuracy"
        )

        # åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        self.performance_monitor = PerformanceMonitor()

        # åŠ è½½æ•°æ®
        logger.info("åŠ è½½æ•°æ®...")
        self.data_loader = NL2SQLDataLoader()
        self.train_examples = self.data_loader.load(self.config.train_file)
        logger.info(f"åŠ è½½äº† {len(self.train_examples)} æ¡è®­ç»ƒæ•°æ®")

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        logger.info(f"åŠ è½½æ¨¡å‹: {self.config.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.config.model_name,
            trust_remote_code=True,
            padding_side="left"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # dtype_map = {
        #     "float32": torch.float32,
        #     "float16": torch.float16,
        #     "bfloat16": torch.bfloat16
        # }
        # torch_dtype = dtype_map.get(self.config.torch_dtype, torch.bfloat16)

        # self.model = AutoModelForCausalLM.from_pretrained(
        #     self.config.model_name,
        #     torch_dtype=torch_dtype,
        #     device_map="auto",
        #     trust_remote_code=True
        # )

        dtype_map = {
            "float32": torch.float32,
            "float16": torch.float16,
            "bfloat16": torch.bfloat16
        }
        torch_dtype = dtype_map.get(self.config.torch_dtype, torch.bfloat16)

        # è¯»å– Qwen3 ç›¸å…³é«˜çº§é…ç½®ï¼ˆç›®å‰ä»…è®°å½•æ—¥å¿—ï¼Œåç»­å¯ç”¨äºç²¾ç»†æ§åˆ¶ï¼‰
        max_seq_length = getattr(self.config, "max_seq_length", None)
        attn_impl = getattr(self.config, "attn_implementation", None)
        rope_scaling_cfg = getattr(self.config, "rope_scaling", None)
        logger.info(
            f"Qwen3 é«˜çº§é…ç½® - max_seq_length={max_seq_length}, "
            f"attn_implementation={attn_impl}, rope_scaling={rope_scaling_cfg}"
        )

        self.model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            torch_dtype=torch_dtype,
            device_map="auto",
            trust_remote_code=True
        )


        # åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹
        self.reward_model = ProcessRewardModel(
            type_weight=self.config.type_weight,
            thinking_weight=self.config.thinking_weight,
            self_assessment_weight=self.config.self_assessment_weight,
            sql_structure_weight=self.config.sql_structure_weight
        )

        logger.info("ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    def _prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # è·å–æ¨¡æ¿
        schema = PromptTemplates.get_schema_context()
        business_knowledge = PromptTemplates.get_business_knowledge()
        few_shot_examples = PromptTemplates.get_few_shot_examples()
        system_prompt = PromptTemplates.BASE_SYSTEM_PROMPT

        train_data = []
        for example in self.train_examples:
            # æ„å»ºç”¨æˆ·å†…å®¹
            user_content = PromptTemplates.SQL_GENERATION_PROMPT.format(
                system_prompt=system_prompt,
                schema=schema,
                business_knowledge=business_knowledge,
                few_shot_examples=few_shot_examples,
                question=example.query,
            )

            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ]

            # ä½¿ç”¨Qwen3èŠå¤©æ¨¡æ¿è½¬æ¢ä¸ºæç¤ºï¼Œå¹¶å¼€å¯thinkingæ¨¡å¼
            prompt = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=True,
            )

            train_data.append({
                "prompt": prompt,
                "reference_type": example.complexity_type,
                "reference_sql": example.sql,
                "query": example.query
            })

        logger.info(f"å‡†å¤‡äº† {len(train_data)} æ¡è®­ç»ƒæ•°æ®")
        return train_data

    def _create_reward_function(self):
        """åˆ›å»ºå¥–åŠ±å‡½æ•°"""
        def compute_rewards(prompts, completions, **kwargs):
            """
            è®¡ç®—å¥–åŠ±

            Args:
                prompts: æç¤ºåˆ—è¡¨
                completions: æ¨¡å‹ç”Ÿæˆçš„å®Œæˆåºåˆ—
                **kwargs: å…¶ä»–å‚æ•°

            Returns:
                å¥–åŠ±å¼ é‡
            """
            rewards = []

            for prompt, completion in zip(prompts, completions):
                # è§£æå®Œæˆä¸­çš„thinkingå’ŒSQL
                thinking = self._extract_section(completion, 'think')
                sql = self._extract_section(completion, 'answer')

                # å¦‚æœæ²¡æœ‰SQLï¼Œç»™äºˆä½å¥–åŠ±
                if not sql:
                    rewards.append(0.0)
                    continue

                # å¯¹äºSQLç±»ä»»åŠ¡ï¼Œè®¡ç®—ç»“æ„å¥–åŠ±
                # ä½¿ç”¨SQLç»“æ„æ£€æŸ¥
                sql_validity = 1.0

                # åŸºç¡€SQLæœ‰æ•ˆæ€§æ£€æŸ¥
                sql_upper = sql.upper()
                if 'SELECT' not in sql_upper or 'FROM' not in sql_upper:
                    sql_validity = 0.0
                elif sql.count('(') != sql.count(')'):
                    sql_validity = 0.5

                # æ¨ç†è´¨é‡æ£€æŸ¥
                thinking_quality = 0.0
                if thinking:
                    # æ£€æŸ¥æ¨ç†é•¿åº¦å’Œå…³é”®è¯
                    if len(thinking) >= 50:
                        thinking_quality += 0.5
                    # æ£€æŸ¥SQLå…³é”®è¯
                    sql_keywords = ['WHERE', 'FROM', 'SELECT', 'JOIN']
                    if any(kw in thinking.upper() for kw in sql_keywords):
                        thinking_quality += 0.3
                    # æ£€æŸ¥é€»è¾‘è¿æ¥è¯
                    logic_words = ['å› ä¸º', 'æ‰€ä»¥', 'ç„¶å', 'é¦–å…ˆ', 'éœ€è¦', 'æ ¹æ®']
                    if any(word in thinking for word in logic_words):
                        thinking_quality += 0.2

                # ç»¼åˆå¥–åŠ±
                total_reward = (
                    0.3 * sql_validity +      # SQLç»“æ„æƒé‡
                    0.3 * thinking_quality  # æ¨ç†è´¨é‡æƒé‡
                )

                rewards.append(total_reward)

            import torch
            import os
            # å¦‚æœæœ‰GPUå¯ç”¨ï¼Œå°†å¥–åŠ±å¼ é‡ç§»åˆ°GPU
            if torch.cuda.is_available():
                device = torch.device("cuda")
            else:
                device = torch.device("cpu")
            return torch.tensor(rewards, device=device)

        return compute_rewards

    @staticmethod
    def _extract_section(text: str, section: str) -> str:
        """æå–æŒ‡å®šæ ‡ç­¾å†…å®¹"""
        start_tag = f"<{section}>"
        end_tag = f"</{section}>"

        start_idx = text.find(start_tag)
        end_idx = text.find(end_tag)

        if start_idx == -1 or end_idx == -1:
            return ""

        content = text[start_idx + len(start_tag):end_idx].strip()
        return content

    def _create_grpo_config(self):
        """åˆ›å»ºGRPOé…ç½®"""
        return GRPOConfig(
            output_dir=self.config.output_dir,
            num_train_epochs=self.config.num_train_epochs,
            per_device_train_batch_size=self.config.per_device_train_batch_size,
            per_device_eval_batch_size=self.config.per_device_eval_batch_size,
            gradient_accumulation_steps=self.config.gradient_accumulation_steps,
            learning_rate=self.config.learning_rate,
            lr_scheduler_type=self.config.lr_scheduler_type,
            warmup_steps=self.config.warmup_steps,
            max_grad_norm=self.config.max_grad_norm,
            weight_decay=self.config.weight_decay,
            bf16=True,
            tf32=True,
            logging_steps=self.config.logging_steps,
            save_steps=self.config.save_steps,
            # ç¦ç”¨è¯„ä¼°ï¼Œå› ä¸ºæ²¡æœ‰eval_dataset
            eval_strategy="no",
            save_strategy="steps",
            report_to=[],  # ç¦ç”¨W&B
            seed=self.config.seed,
            remove_unused_columns=False,
            dataloader_pin_memory=True,
            dataloader_num_workers=8,
        )


    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("=" * 50)
        logger.info("å¼€å§‹GRPOè®­ç»ƒ")
        logger.info("=" * 50)

        self.performance_monitor.start()

        try:
            # å‡†å¤‡æ•°æ®
            train_data = self._prepare_training_data()

            # åˆ›å»ºGRPOé…ç½®
            grpo_config = self._create_grpo_config()

            # åˆ›å»ºTensorBoardå›è°ƒ
            from utils.tensorboard_callback import TensorBoardCallback
            tb_callback = TensorBoardCallback(self.tb_logger)

            # åˆ›å»ºGRPOè®­ç»ƒå™¨
            trainer = GRPOTrainer(
                model=self.model,
                args=grpo_config,
                train_dataset=train_data,
                reward_funcs=[self._create_reward_function()],
                processing_class=self.tokenizer,
                callbacks=[tb_callback],
            )

            # å¼€å§‹è®­ç»ƒ
            logger.info("è®­ç»ƒå¼€å§‹...")
            train_result = trainer.train(resume_from_checkpoint=self.config.resume_from_checkpoint)

            # è®°å½•è®­ç»ƒç»“æœ
            logger.info("=" * 50)
            logger.info("è®­ç»ƒå®Œæˆ")
            logger.info(f"æœ€ç»ˆæŸå¤±: {train_result.training_loss}")
            logger.info("=" * 50)

            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_dir = Path(self.config.output_dir) / "final_model"
            self.model.save_pretrained(final_model_dir)
            self.tokenizer.save_pretrained(final_model_dir)
            logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_dir}")

            # ä¸Šä¼ åˆ°W&B
            if self.config.wandb_enabled:
                self.wandb_logger.log_model(str(final_model_dir), "final_model")

            # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
            self._save_performance_report()

        except Exception as e:
            logger.error(f"è®­ç»ƒå‡ºé”™: {e}")
            raise
        finally:
            # å®ŒæˆTensorBoardè®°å½•
            logger.info("æ­£åœ¨ä¿å­˜è®­ç»ƒæ—¥å¿—...")
            self.tb_logger.finish()

            # åœæ­¢TensorBoardæœåŠ¡å™¨ï¼ˆå¦‚æœæ˜¯è‡ªåŠ¨å¯åŠ¨çš„ï¼‰
            self.tb_logger.stop_tensorboard_server()

            # å®ŒæˆW&B
            if self.config.wandb_enabled and self.wandb_logger:
                self.wandb_logger.finish()

    def _save_performance_report(self):
        """ä¿å­˜æ€§èƒ½æŠ¥å‘Š"""
        report = self.performance_monitor.get_metrics_summary()
        report_path = Path(self.config.output_dir) / "performance_report.txt"

        with open(report_path, 'w') as f:
            f.write("=" * 50 + "\n")
            f.write("NL2SQL GRPOè®­ç»ƒæ€§èƒ½æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")

            for key, value in report.items():
                f.write(f"{key}: {value}\n")

            f.write("\n" + "=" * 50 + "\n")
            f.write("æ•°æ®é›†ç»Ÿè®¡\n")
            f.write("=" * 50 + "\n")

            stats = self.data_loader.get_statistics()
            for key, value in stats.items():
                f.write(f"{key}: {value}\n")

        logger.info(f"æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def load_config(config_path: str) -> TrainingConfig:
    """ä»YAMLåŠ è½½é…ç½®"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config_dict = yaml.safe_load(f)

    # å±•å¹³åµŒå¥—å­—å…¸
    training_config = config_dict.get('training', {})
    model_config = config_dict.get('model', {})
    data_config = config_dict.get('data', {})
    grpo_config = config_dict.get('grpo', {})
    wandb_config = config_dict.get('wandb', {})

    # åˆå¹¶æ‰€æœ‰é…ç½®
    merged_config = {
        **training_config,
        **model_config,
        **data_config,
        **grpo_config,
        **wandb_config,
    }

    # æå–reward_weights
    reward_weights = grpo_config.get('reward_weights', {})
    merged_config['type_weight'] = reward_weights.get('type_reward', 0.20)
    merged_config['thinking_weight'] = reward_weights.get('thinking_reward', 0.25)
    merged_config['self_assessment_weight'] = reward_weights.get('self_assessment_reward', 0.25)
    merged_config['sql_structure_weight'] = reward_weights.get('sql_structure_reward', 0.30)

    # ç­›é€‰å‡ºTrainingConfigä¸­å®šä¹‰çš„å­—æ®µ
    valid_fields = set(TrainingConfig.__dataclass_fields__.keys())
    filtered_config = {k: v for k, v in merged_config.items() if k in valid_fields}

    print(f"ğŸ“ æœ‰æ•ˆé…ç½®å­—æ®µ: {list(filtered_config.keys())}")

    return TrainingConfig(**filtered_config)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="NL2SQL GRPOè®­ç»ƒè„šæœ¬")
    parser.add_argument(
        "--config",
        type=str,
        default="./config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--resume",
        type=str,
        default=None,
        help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„"
    )
    # Add GPU configuration arguments
    parser.add_argument(
        "--cuda_visible_devices",
        type=str,
        default=None,
        help="CUDA visible devices (e.g., 0,1,2,3)"
    )
    parser.add_argument(
        "--gpus_per_node",
        type=int,
        default=None,
        help="Number of GPUs per node"
    )
    parser.add_argument(
        "--tensor_model_parallel_size",
        type=int,
        default=None,
        help="Tensor model parallel size"
    )

    args = parser.parse_args()

    # Set GPU-related environment variables
    if args.cuda_visible_devices:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.cuda_visible_devices
        print(f"ğŸ”§ Setting CUDA_VISIBLE_DEVICES={args.cuda_visible_devices}")

    if args.gpus_per_node:
        os.environ["N_GPUS_PER_NODE"] = str(args.gpus_per_node)
        print(f"ğŸ”§ Setting N_GPUS_PER_NODE={args.gpus_per_node}")

    if args.tensor_model_parallel_size:
        os.environ["TENSOR_MODEL_PARALLEL_SIZE"] = str(args.tensor_model_parallel_size)
        print(f"ğŸ”§ Setting TENSOR_MODEL_PARALLEL_SIZE={args.tensor_model_parallel_size}")

    # åŠ è½½é…ç½®
    config = load_config(args.config)
    if args.resume:
        config.resume_from_checkpoint = args.resume

    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    trainer = NL2SQLTrainer(config)
    trainer.train()


if __name__ == "__main__":
    main()
